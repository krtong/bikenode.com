{
    "sourceFile": "data/bicycles/scrape.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1740843152396,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1740843291536,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,13 +81,13 @@\n             \"url\": url,\n             \"element_counts\": {},\n             \"common_classes\": {},\n             \"potential_bike_elements\": [],\n-            \"potential_selectors\": []\n+            \"potential_selectors\": [],\n+            \"interesting_elements\": []\n         }\n         \n         # Get counts of common elements\n-        element_types = ['div', 'a', 'span', 'h1', 'h2', 'h3', 'article', 'section', 'li', 'ul']\n         for el_type in element_types:\n             elements = self.driver.find_elements(By.TAG_NAME, el_type)\n             dom_info[\"element_counts\"][el_type] = len(elements)\n         \n"
                },
                {
                    "date": 1740843298856,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,20 +115,20 @@\n         # For example, elements containing both an image and price-like text\n         logger.info(\"Looking for content patterns that might indicate bike listings...\")\n         \n         # 1. Find elements containing images\n-            elements = self.driver.find_elements(By.XPATH, xpath)\n-            \n-            if elements:\n-                selector_info = {\n-                    \"selector\": f\".{term}\",\n-                    \"count\": len(elements),\n-                    \"samples\": []\n-                }\n+        elements_with_images = self.driver.find_elements(By.XPATH, \"//div[.//img]\")\n+        logger.info(f\"Found {len(elements_with_images)} elements containing images\")\n+        \n+        # 2. Find elements that might be product cards based on content\n+        for i, element in enumerate(elements_with_images[:20]):\n+            try:\n+                # Check if this element has characteristics of a product card\n+                image_elements = element.find_elements(By.TAG_NAME, \"img\")\n+                text_content = element.text.strip()\n                 \n-                # Get sample elements\n-                for i, el in enumerate(elements[:sample_elements]):\n-                    try:\n+                # Skip very small or empty elements\n+                if len(text_content) < 5 or element.size['height'] < 100:\n                         el_info = {\n                             \"tag\": el.tag_name,\n                             \"class\": el.get_attribute(\"class\"),\n                             \"id\": el.get_attribute(\"id\"),\n"
                },
                {
                    "date": 1740843306359,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,26 +131,26 @@\n                     continue\n                 \n                 # If it has an image and some text, it might be interesting\n                 element_info = {\n-                            \"text_snippet\": el.text[:100] + \"...\" if len(el.text) > 100 else el.text\n-                        }\n-                        selector_info[\"samples\"].append(el_info)\n-                    except:\n-                        pass\n+                    \"index\": i,\n+                    \"tag\": element.tag_name,\n+                    \"class\": element.get_attribute(\"class\"),\n+                    \"id\": element.get_attribute(\"id\"),\n+                    \"text_content\": text_content[:200] + \"...\" if len(text_content) > 200 else text_content,\n+                    \"image_count\": len(image_elements),\n+                    \"xpath\": self._generate_xpath(element),\n+                    \"size\": element.size\n+                }\n                 \n-                dom_info[\"potential_bike_elements\"].append(selector_info)\n+                dom_info[\"interesting_elements\"].append(element_info)\n+            except Exception as e:\n+                logger.error(f\"Error analyzing element {i}: {e}\")\n+                \n+        # 3. Look specifically for product grid patterns\n+        grid_containers = self.driver.find_elements(By.XPATH, \n+            \"//div[count(.//div) > 5 and count(.//img) > 3]\")\n         \n-        # Look for common product grid patterns\n-        grid_selectors = [\n-            \".grid\", \".product-grid\", \".search-results\", \".listing\", \n-            \".products-grid\", \".collection-grid\", \"ul.products\"\n-        ]\n-        \n-        for selector in grid_selectors:\n-            try:\n-                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n-                if elements:\n                     dom_info[\"potential_selectors\"].append({\n                         \"selector\": selector,\n                         \"count\": len(elements)\n                     })\n"
                }
            ],
            "date": 1740843152396,
            "name": "Commit-0",
            "content": "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\nimport csv\nimport json\nimport logging\nimport os\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass NinetyNineSpokesScraper:\n    def __init__(self, headless=False, debug_dir=\"debug_output\"):\n        \"\"\"Initialize the scraper with optional headless mode\"\"\"\n        options = webdriver.ChromeOptions()\n        if headless:\n            options.add_argument('--headless')\n        self.driver = webdriver.Chrome(options=options)\n        self.bikes = []\n        \n        # Create debug directory if it doesn't exist\n        self.debug_dir = debug_dir\n        os.makedirs(self.debug_dir, exist_ok=True)\n    \n    def debug_page(self, page_num=1, prefix=\"debug\"):\n        \"\"\"Capture current page state for debugging\"\"\"\n        try:\n            # Take screenshot\n            screenshot_path = os.path.join(self.debug_dir, f\"{prefix}_page{page_num}.png\")\n            self.driver.save_screenshot(screenshot_path)\n            logger.info(f\"Screenshot saved to {screenshot_path}\")\n            \n            # Save HTML content\n            html_path = os.path.join(self.debug_dir, f\"{prefix}_page{page_num}.html\")\n            with open(html_path, 'w', encoding='utf-8') as f:\n                f.write(self.driver.page_source)\n            logger.info(f\"HTML source saved to {html_path}\")\n            \n            # Log basic page info\n            logger.info(f\"Page title: {self.driver.title}\")\n            \n            # Print some common elements to see what's available\n            element_types = ['div', 'a', 'span', 'h1', 'h2', 'article']\n            for el_type in element_types:\n                elements = self.driver.find_elements(By.TAG_NAME, el_type)\n                logger.info(f\"Found {len(elements)} {el_type} elements\")\n                \n            # Look for elements with common class name patterns\n            for class_pattern in ['bike', 'product', 'listing', 'card', 'item']:\n                xpath = f\"//*[contains(@class, '{class_pattern}')]\"\n                elements = self.driver.find_elements(By.XPATH, xpath)\n                logger.info(f\"Found {len(elements)} elements with class containing '{class_pattern}'\")\n                if len(elements) > 0 and len(elements) < 10:\n                    for i, el in enumerate(elements[:3]):\n                        logger.info(f\"  Element {i} class: {el.get_attribute('class')}\")\n            \n            return True\n        except Exception as e:\n            logger.error(f\"Error during debug: {e}\")\n            return False\n\n    def analyze_dom_structure(self, url, sample_elements=3):\n        \"\"\"\n        Analyze the DOM structure of a given page to help determine selectors\n        \n        Args:\n            url: The URL to analyze\n            sample_elements: Number of sample elements to show for each selector\n            \n        Returns:\n            Dictionary with DOM structure information\n        \"\"\"\n        logger.info(f\"Analyzing DOM structure of: {url}\")\n        self.driver.get(url)\n        time.sleep(5)  # Wait for page to fully load\n        \n        dom_info = {\n            \"page_title\": self.driver.title,\n            \"url\": url,\n            \"element_counts\": {},\n            \"common_classes\": {},\n            \"potential_bike_elements\": [],\n            \"potential_selectors\": []\n        }\n        \n        # Get counts of common elements\n        element_types = ['div', 'a', 'span', 'h1', 'h2', 'h3', 'article', 'section', 'li', 'ul']\n        for el_type in element_types:\n            elements = self.driver.find_elements(By.TAG_NAME, el_type)\n            dom_info[\"element_counts\"][el_type] = len(elements)\n        \n        # Find the most common classes\n        all_elements = self.driver.find_elements(By.XPATH, \"//*[@class]\")\n        class_count = {}\n        \n        for element in all_elements:\n            try:\n                classes = element.get_attribute(\"class\").split()\n                for cls in classes:\n                    if cls in class_count:\n                        class_count[cls] += 1\n                    else:\n                        class_count[cls] = 1\n            except:\n                pass\n        \n        # Sort classes by frequency\n        sorted_classes = sorted(class_count.items(), key=lambda x: x[1], reverse=True)\n        dom_info[\"common_classes\"] = {cls: count for cls, count in sorted_classes[:20]}\n        \n        # Check for potential bike listing elements\n        bike_related_terms = [\"bike\", \"product\", \"listing\", \"card\", \"item\", \"bicycle\", \"result\"]\n        \n        for term in bike_related_terms:\n            # Check class names\n            xpath = f\"//*[contains(@class, '{term}')]\"\n            elements = self.driver.find_elements(By.XPATH, xpath)\n            \n            if elements:\n                selector_info = {\n                    \"selector\": f\".{term}\",\n                    \"count\": len(elements),\n                    \"samples\": []\n                }\n                \n                # Get sample elements\n                for i, el in enumerate(elements[:sample_elements]):\n                    try:\n                        el_info = {\n                            \"tag\": el.tag_name,\n                            \"class\": el.get_attribute(\"class\"),\n                            \"id\": el.get_attribute(\"id\"),\n                            \"text_snippet\": el.text[:100] + \"...\" if len(el.text) > 100 else el.text\n                        }\n                        selector_info[\"samples\"].append(el_info)\n                    except:\n                        pass\n                \n                dom_info[\"potential_bike_elements\"].append(selector_info)\n        \n        # Look for common product grid patterns\n        grid_selectors = [\n            \".grid\", \".product-grid\", \".search-results\", \".listing\", \n            \".products-grid\", \".collection-grid\", \"ul.products\"\n        ]\n        \n        for selector in grid_selectors:\n            try:\n                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n                if elements:\n                    dom_info[\"potential_selectors\"].append({\n                        \"selector\": selector,\n                        \"count\": len(elements)\n                    })\n            except:\n                pass\n        \n        # Take a screenshot for reference\n        screenshot_path = os.path.join(self.debug_dir, \"dom_analysis.png\")\n        self.driver.save_screenshot(screenshot_path)\n        \n        # Save HTML source\n        html_path = os.path.join(self.debug_dir, \"dom_analysis.html\")\n        with open(html_path, 'w', encoding='utf-8') as f:\n            f.write(self.driver.page_source)\n        \n        logger.info(f\"DOM analysis complete. Found {len(dom_info['potential_bike_elements'])} potential bike element types\")\n        return dom_info\n\n    def search_bikes(self, year=2025, max_pages=5, debug=False):\n        \"\"\"\n        Search for bikes on 99spokes.com\n        \n        Args:\n            year: Model year to search for\n            max_pages: Maximum number of pages to scrape\n            debug: Whether to run in debug mode\n            \n        Returns:\n            List of dictionaries with bike data\n        \"\"\"\n        self.bikes = []\n        page = 1\n        \n        while page <= max_pages:\n            # Construct the URL for the current page\n            url = f\"https://99spokes.com/bikes?year={year}&page={page}\"\n            logger.info(f\"Scraping page {page} from: {url}\")\n            self.driver.get(url)\n            \n            # Add a longer wait time\n            time.sleep(5)  # Wait for JavaScript to render content\n            \n            if debug:\n                self.debug_page(page)\n            \n            # Try multiple selectors based on common patterns\n            bike_elements = []\n            possible_selectors = [\n                \".bike-card\", \".bike-listing\", \".product-card\", \n                \".product-item\", \"article\", \"[data-testid='bike-card']\",\n                \".product-grid-item\", \".search-result-item\"\n            ]\n            \n            for selector in possible_selectors:\n                try:\n                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n                    if elements:\n                        logger.info(f\"Found {len(elements)} bikes using selector: {selector}\")\n                        bike_elements = elements\n                        break\n                except Exception as e:\n                    logger.debug(f\"Selector {selector} failed: {e}\")\n            \n            if not bike_elements:\n                logger.warning(\"No bike elements found with any selector.\")\n                if not debug:\n                    # Try debug mode to help identify the correct selectors\n                    logger.info(\"Running debug to analyze page structure...\")\n                    self.debug_page(page, \"failed_search\")\n                break\n            \n            # Extract data from each bike listing - using more flexible approach\n            for bike_element in bike_elements:\n                bike_data = {\"year\": year}\n                \n                # Try multiple ways to extract make/brand\n                for make_selector in [\".make\", \".brand\", \"[data-testid='bike-brand']\", \"h3\"]:\n                    try:\n                        make = bike_element.find_element(By.CSS_SELECTOR, make_selector).text.strip()\n                        if make:\n                            bike_data[\"make\"] = make\n                            break\n                    except:\n                        pass\n                \n                if \"make\" not in bike_data:\n                    bike_data[\"make\"] = \"Unknown\"\n                \n                # Try multiple ways to extract model\n                for model_selector in [\".model\", \".name\", \"[data-testid='bike-model']\", \"h2\"]:\n                    try:\n                        model = bike_element.find_element(By.CSS_SELECTOR, model_selector).text.strip()\n                        if model:\n                            bike_data[\"model\"] = model\n                            break\n                    except:\n                        pass\n                \n                if \"model\" not in bike_data:\n                    bike_data[\"model\"] = \"Unknown\"\n                \n                # Try multiple ways to extract type/category\n                for type_selector in [\".type\", \".category\", \"[data-testid='bike-category']\", \".bike-type\"]:\n                    try:\n                        type_ = bike_element.find_element(By.CSS_SELECTOR, type_selector).text.strip()\n                        if type_:\n                            bike_data[\"type\"] = type_\n                            break\n                    except:\n                        pass\n                \n                if \"type\" not in bike_data:\n                    bike_data[\"type\"] = \"Unknown\"\n                \n                # Try to get href/URL if available\n                try:\n                    link = bike_element.find_element(By.CSS_SELECTOR, \"a\")\n                    bike_data[\"url\"] = link.get_attribute(\"href\")\n                except:\n                    bike_data[\"url\"] = None\n                \n                self.bikes.append(bike_data)\n            \n            logger.info(f\"Scraped page {page}, total bikes: {len(self.bikes)}\")\n            page += 1\n            time.sleep(2)  # Delay to avoid overwhelming the server\n            \n        return self.bikes\n    \n    def get_bike_details(self, bike_url):\n        \"\"\"\n        Get detailed information from a specific bike listing\n        \n        Args:\n            bike_url: URL of the bike to scrape\n            \n        Returns:\n            Dictionary with bike details\n        \"\"\"\n        logger.info(f\"Getting bike details from: {bike_url}\")\n        self.driver.get(bike_url)\n        \n        try:\n            # Wait for bike details to load\n            WebDriverWait(self.driver, 10).until(\n                EC.presence_of_element_located((By.CLASS_NAME, \"bike-details\"))\n            )\n            \n            bike_data = {}\n            \n            # Basic info\n            try:\n                bike_data[\"title\"] = self.driver.find_element(By.CLASS_NAME, \"bike-title\").text\n            except:\n                bike_data[\"title\"] = \"Unknown\"\n                \n            try:\n                bike_data[\"price\"] = self.driver.find_element(By.CLASS_NAME, \"bike-price\").text\n            except:\n                bike_data[\"price\"] = \"Not listed\"\n            \n            # Description\n            try:\n                bike_data[\"description\"] = self.driver.find_element(By.CLASS_NAME, \"bike-description\").text\n            except:\n                bike_data[\"description\"] = \"No description available\"\n            \n            # Specifications\n            try:\n                specs = {}\n                spec_elements = self.driver.find_elements(By.CSS_SELECTOR, \".bike-specs .spec-item\")\n                \n                for spec in spec_elements:\n                    key_elem = spec.find_element(By.CLASS_NAME, \"spec-key\")\n                    value_elem = spec.find_element(By.CLASS_NAME, \"spec-value\")\n                    specs[key_elem.text.strip()] = value_elem.text.strip()\n                    \n                bike_data[\"specifications\"] = specs\n            except:\n                bike_data[\"specifications\"] = {}\n                \n            return bike_data\n            \n        except Exception as e:\n            logger.error(f\"Error getting bike details: {e}\")\n            return {\"error\": str(e)}\n    \n    def save_to_csv(self, filename=\"bikes_data.csv\"):\n        \"\"\"Save scraped bike data to CSV\"\"\"\n        if not self.bikes:\n            logger.warning(\"No bikes to save\")\n            return False\n            \n        try:\n            # Get all possible fieldnames from all bikes\n            all_fields = set()\n            for bike in self.bikes:\n                all_fields.update(bike.keys())\n            \n            fieldnames = sorted(list(all_fields))\n            \n            with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n                for bike in self.bikes:\n                    writer.writerow(bike)\n            logger.info(f\"Saved {len(self.bikes)} bikes to {filename}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Error saving to CSV: {e}\")\n            return False\n    \n    def bike_to_json(self, bike_data):\n        \"\"\"Convert bike data to JSON string\"\"\"\n        return json.dumps(bike_data, indent=2)\n    \n    def close(self):\n        \"\"\"Close the browser\"\"\"\n        self.driver.quit()\n\n\n# Example usage when script is run directly\nif __name__ == \"__main__\":\n    scraper = NinetyNineSpokesScraper(headless=False)\n    try:\n        # First analyze the DOM structure\n        logger.info(\"Analyzing DOM structure to identify correct selectors...\")\n        dom_info = scraper.analyze_dom_structure(\"https://99spokes.com/bikes?year=2025\")\n        \n        # Print some of the analysis results\n        print(\"\\n==== DOM ANALYSIS RESULTS ====\")\n        print(f\"Page title: {dom_info['page_title']}\")\n        \n        print(\"\\nPotential bike element selectors:\")\n        for selector_info in dom_info['potential_bike_elements']:\n            print(f\"  {selector_info['selector']}: {selector_info['count']} elements\")\n            \n        print(\"\\nMost common classes:\")\n        for cls, count in list(dom_info['common_classes'].items())[:10]:\n            print(f\"  .{cls}: {count} elements\")\n            \n        print(\"\\nSaved full analysis and page source to debug_output directory\")\n        \n        # Then proceed with scraping if needed\n        proceed = input(\"\\nProceed with scraping based on this analysis? (y/n): \")\n        if proceed.lower() == 'y':\n            logger.info(\"Starting main scraping process...\")\n            bikes = scraper.search_bikes(year=2025, max_pages=3)\n            \n            if bikes:\n                scraper.save_to_csv(\"bikes_2025.csv\")\n    finally:\n        scraper.close()"
        }
    ]
}