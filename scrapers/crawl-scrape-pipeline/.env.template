# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=bikenode
DB_USER=postgres
DB_PASSWORD=your_password_here

# Crawler Settings
USER_AGENT=Mozilla/5.0 (compatible; BikeNodeBot/1.0; +http://bikenode.com/bot)
CONCURRENT_REQUESTS=32
CONCURRENT_REQUESTS_PER_DOMAIN=16
DOWNLOAD_DELAY=1
RANDOMIZE_DOWNLOAD_DELAY=true
ROBOTSTXT_OBEY=true
COOKIES_ENABLED=false
DEPTH_LIMIT=3
CLOSESPIDER_PAGECOUNT=1000

# Processing Settings
BATCH_SIZE=1000
MAX_RETRIES=3
TIMEOUT=30
CHUNK_SIZE=1000

# Quality Control Settings
MIN_DATA_FRESHNESS_DAYS=7
PRICE_STABILITY_THRESHOLD=0.5
MIN_COMPLETENESS_RATIO=0.8

# Logging Settings
LOG_LEVEL=INFO
LOG_FILE=logs/pipeline.log

# Optional: Screaming Frog Path (if available)
SCREAMING_FROG_PATH=/Applications/Screaming Frog SEO Spider.app/Contents/Resources/app/ScreamingFrogSEOSpider.jar

# Optional: Proxy Settings
# PROXY_HOST=proxy.example.com
# PROXY_PORT=8080
# PROXY_USER=username
# PROXY_PASS=password

# Optional: AWS S3 Storage (for large-scale HTML storage)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# S3_BUCKET_NAME=bikenode-crawler-data
# S3_REGION=us-east-1

# Optional: Monitoring (Prometheus/Grafana)
# METRICS_PORT=9090
# ENABLE_METRICS=true