#!/usr/bin/env node
const BennettsScraper = require('./scrapers/bennetts_scraper.js');
const BennettsDatabase = require('./scrapers/bennetts_database.js');
const config = require('./scrapers/config.js');

async function runProduction() {
    console.log('ğŸï¸  Bennetts Production Scraper');
    console.log('================================\n');
    
    // Parse command line arguments
    const args = process.argv.slice(2);
    const maxReviews = args.length > 0 ? parseInt(args[0]) : null;
    const maxPages = args.length > 1 ? parseInt(args[1]) : null;
    
    if (maxReviews) {
        console.log(`ğŸ¯ Limiting to ${maxReviews} reviews`);
    }
    if (maxPages) {
        console.log(`ğŸ“„ Limiting to ${maxPages} pages`);
    }
    
    const scraper = new BennettsScraper();
    const db = new BennettsDatabase();
    
    let startTime = Date.now();
    
    try {
        // Initialize
        console.log('\nğŸš€ Initializing...');
        await db.init();
        await scraper.initBrowser();
        console.log('âœ… Ready to scrape\n');
        
        // Get all review URLs
        console.log('ğŸ“‹ Collecting review URLs...');
        const allUrls = await scraper.getAllReviewUrls(maxPages);
        const urlsToScrape = maxReviews ? allUrls.slice(0, maxReviews) : allUrls;
        
        console.log(`ğŸ¯ Will scrape ${urlsToScrape.length} out of ${allUrls.length} total reviews\n`);
        
        // Scrape reviews
        let successCount = 0;
        let errorCount = 0;
        
        for (let i = 0; i < urlsToScrape.length; i++) {
            const url = urlsToScrape[i];
            const progress = `${i + 1}/${urlsToScrape.length}`;
            
            console.log(`ğŸ“– [${progress}] Processing review...`);
            
            try {
                const reviewData = await scraper.scrapeReviewDetails(url);
                
                if (reviewData) {
                    await db.insertReview(reviewData);
                    successCount++;
                    console.log(`âœ… [${progress}] ${reviewData.title}`);
                } else {
                    errorCount++;
                    console.log(`âŒ [${progress}] Failed to extract data from ${url}`);
                }
                
                // Save progress every 5 reviews
                if ((i + 1) % 5 === 0) {
                    await scraper.saveData();
                    console.log(`ğŸ’¾ Progress saved (${successCount} successful, ${errorCount} failed)`);
                }
                
                // Add delay between reviews
                if (i < urlsToScrape.length - 1) {
                    await new Promise(resolve => setTimeout(resolve, config.scraping.delays.betweenReviews));
                }
                
            } catch (error) {
                errorCount++;
                console.error(`âŒ [${progress}] Error: ${error.message}`);
            }
        }
        
        // Download images
        console.log(`\nğŸ“¸ Downloading images...`);
        await scraper.downloadAllImages();
        
        // Final save
        console.log('\nğŸ’¾ Saving final data...');
        await scraper.saveData();
        
        // Show final statistics
        const stats = await db.getStats();
        const duration = Math.round((Date.now() - startTime) / 1000);
        
        console.log('\nğŸ‰ Scraping completed!');
        console.log(`â±ï¸  Duration: ${duration} seconds`);
        console.log(`âœ… Successful: ${successCount}`);
        console.log(`âŒ Failed: ${errorCount}`);
        console.log(`ğŸ“Š Total in database: ${stats.totalReviews} reviews`);
        console.log(`ğŸ­ Manufacturers: ${stats.totalManufacturers}`);
        console.log(`ğŸï¸  Models: ${stats.totalModels}`);
        console.log(`ğŸ‘¥ Authors: ${stats.totalAuthors}`);
        console.log(`ğŸ“¸ Images: ${stats.totalImages}`);
        
        console.log('\nğŸ“ Data saved to:');
        console.log(`   Database: ${config.database.path}`);
        console.log(`   JSON: ${config.output.directories.reviews}/all_reviews.json`);
        console.log(`   Images: ${config.output.directories.images}/`);
        
    } catch (error) {
        console.error('\nğŸ’¥ Fatal error:', error.message);
        console.error(error.stack);
        process.exit(1);
    } finally {
        console.log('\nğŸ§¹ Cleaning up...');
        await scraper.closeBrowser();
        await db.close();
        console.log('âœ… Cleanup complete');
        
        process.exit(0);
    }
}

// Show usage if no valid arguments
if (require.main === module) {
    console.log('Usage: node run_scraper.js [maxReviews] [maxPages]');
    console.log('Examples:');
    console.log('  node run_scraper.js          # Scrape all reviews');
    console.log('  node run_scraper.js 50       # Scrape first 50 reviews');
    console.log('  node run_scraper.js 50 3     # Scrape first 50 reviews from first 3 pages');
    console.log('');
    
    runProduction();
}
